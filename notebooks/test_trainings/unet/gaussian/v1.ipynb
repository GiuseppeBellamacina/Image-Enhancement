{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b42cc00",
   "metadata": {},
   "source": [
    "# UNet Training - Gaussian Noise Removal\n",
    "\n",
    "Questo notebook addestra una rete UNet per rimuovere il rumore gaussiano dalle immagini.\n",
    "\n",
    "## üîÑ Resume Training\n",
    "\n",
    "Il notebook supporta la ripresa dell'addestramento da un checkpoint precedente:\n",
    "\n",
    "- **`resume_from_checkpoint`**: Imposta a `True` per riprendere da un checkpoint esistente\n",
    "- **`resume_experiment`**: Specifica quale esperimento caricare:\n",
    "  - `\"latest\"` - Carica l'esperimento pi√π recente\n",
    "  - `\"20251229_224726\"` - Carica un esperimento specifico per timestamp\n",
    "\n",
    "Quando riprendi l'addestramento, vengono automaticamente caricati:\n",
    "- Stato del modello\n",
    "- Stato dell'optimizer\n",
    "- Stato dello scheduler\n",
    "- Epoca di partenza\n",
    "- Best validation loss precedente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae29996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from src.degradations.generate_degraded_dataset import generate_degraded_dataset\n",
    "from src.losses.combined_loss import CombinedLoss\n",
    "from src.models.unet import UNet\n",
    "from src.training import get_dataloaders, run_training\n",
    "from src.utils import (\n",
    "    get_degraded_data_dir,\n",
    "    get_raw_data_dir,\n",
    "    load_checkpoint,\n",
    "    plot_image_comparison,\n",
    "    plot_inference_results,\n",
    "    plot_training_curves,\n",
    "    print_training_summary,\n",
    "    save_training_history,\n",
    "    setup_experiment,\n",
    "    resume_training,\n",
    ")\n",
    "from src.evaluation import ImageRestorationEvaluator\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd7a2e",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Resume Training\n",
    "    \"resume_from_checkpoint\": False,  # Set to True to resume from latest checkpoint\n",
    "    \"resume_experiment\": \"latest\",     # \"latest\" or specific timestamp like \"20251229_224726\"\n",
    "    # Data\n",
    "    \"train_degraded_dir\": str(get_degraded_data_dir() / \"gaussian\" / \"DIV2K_train_HR\"),\n",
    "    \"train_clean_dir\": str(get_raw_data_dir() / \"DIV2K_train_HR\"),\n",
    "    \"val_degraded_dir\": str(get_degraded_data_dir() / \"gaussian\" / \"DIV2K_valid_HR\"),\n",
    "    \"val_clean_dir\": str(get_raw_data_dir() / \"DIV2K_valid_HR\"),\n",
    "    # Training\n",
    "    \"batch_size\": 16,\n",
    "    \"num_epochs\": 35,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    # Data\n",
    "    \"patch_size\": 128,\n",
    "    \"patches_per_image\": 10,\n",
    "    \"num_workers\": 4,\n",
    "    # Model\n",
    "    \"model_features\": 64,\n",
    "    \"model_bilinear\": True,\n",
    "    # Loss\n",
    "    \"loss_alpha\": 0.84,  # L1 weight\n",
    "    \"loss_beta\": 0.16,  # SSIM weight\n",
    "    # Degradation\n",
    "    \"noise_sigma\": 100.0,  # Gaussian noise standard deviation\n",
    "    # Optimization\n",
    "    \"scheduler\": \"cosine\",\n",
    "    \"warmup_epochs\": 5,\n",
    "    \"min_lr\": 1e-6,\n",
    "    # Early stopping\n",
    "    \"patience\": 5,\n",
    "    # Checkpoints\n",
    "    \"save_every\": 5,\n",
    "    \"val_every\": 1,\n",
    "    # Device\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config[\"seed\"])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "print(\"\\nüìã Configuration:\")    print(f\"   {key}: {value}\")\n",
    "for key, value in config.items():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d22e0",
   "metadata": {},
   "source": [
    "## 3. Create Output Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a00703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup experiment directory (will auto-detect project root)\n",
    "exp_dir, subdirs = setup_experiment(\n",
    "    model_name=\"unet\",\n",
    "    degradation=\"gaussian\",\n",
    "    config=config,\n",
    "    custom_name=\"v1\"  # Optional: adds a custom suffix to experiment name\n",
    ")\n",
    "\n",
    "logs_dir = subdirs[\"logs\"]\n",
    "\n",
    "# Extract subdirectory pathssamples_dir = subdirs[\"samples\"]\n",
    "checkpoints_dir = subdirs[\"checkpoints\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a2157",
   "metadata": {},
   "source": [
    "## 4. Setup TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a30aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=logs_dir)\n",
    "\n",
    "print(f\"\\nüìä TensorBoard logs: {logs_dir}\")\n",
    "print(f\"   Run: tensorboard --logdir {logs_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7513a2",
   "metadata": {},
   "source": [
    "## 5. Generate Degraded Datasets\n",
    "\n",
    "Generate corrupted versions of DIV2K images using Gaussian noise with sigma=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddcf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if degraded datasets already exist\n",
    "train_degraded_exists = Path(config[\"train_degraded_dir\"]).exists()\n",
    "val_degraded_exists = Path(config[\"val_degraded_dir\"]).exists()\n",
    "\n",
    "if train_degraded_exists and val_degraded_exists:\n",
    "    n_train = len(list(Path(config[\"train_degraded_dir\"]).glob(\"*.png\")))\n",
    "    n_val = len(list(Path(config[\"val_degraded_dir\"]).glob(\"*.png\")))\n",
    "    print(\"‚úÖ Degraded datasets already exist:\")\n",
    "    print(f\"   Train: {n_train} images in {config['train_degraded_dir']}\")\n",
    "    print(f\"   Val: {n_val} images in {config['val_degraded_dir']}\")\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping generation (delete folders to regenerate)\")\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üé® Generating Degraded Datasets\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nDegradation: Gaussian noise (sigma={config['noise_sigma']})\")\n",
    "    print(\"This will create corrupted versions of DIV2K images\\n\")\n",
    "\n",
    "    # Generate training dataset\n",
    "    if not train_degraded_exists:\n",
    "        print(\"üìÇ Training Dataset\")\n",
    "        generate_degraded_dataset(\n",
    "            input_dir=config[\"train_clean_dir\"],\n",
    "            output_dir=config[\"train_degraded_dir\"],\n",
    "            degradation_type=\"gaussian_noise\",\n",
    "            noise_sigma=config[\"noise_sigma\"],\n",
    "            seed=config[\"seed\"],\n",
    "        )\n",
    "\n",
    "    # Generate validation dataset\n",
    "    if not val_degraded_exists:\n",
    "        print(\"üìÇ Validation Dataset\")\n",
    "        generate_degraded_dataset(\n",
    "            input_dir=config[\"val_clean_dir\"],\n",
    "            output_dir=config[\"val_degraded_dir\"],\n",
    "            degradation_type=\"gaussian_noise\",\n",
    "            noise_sigma=config[\"noise_sigma\"],\n",
    "            seed=config[\"seed\"],\n",
    "        )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ Degraded datasets generated successfully!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657c75e",
   "metadata": {},
   "source": [
    "## 6. Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba88e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    train_degraded_dir=config[\"train_degraded_dir\"],\n",
    "    train_clean_dir=config[\"train_clean_dir\"],\n",
    "    val_degraded_dir=config[\"val_degraded_dir\"],\n",
    "    val_clean_dir=config[\"val_clean_dir\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    patch_size=config[\"patch_size\"],\n",
    "    patches_per_image=config[\"patches_per_image\"],\n",
    "    num_workers=config[\"num_workers\"],\n",
    ")\n",
    "\n",
    "print(f\"\\n   Batches per epoch: {len(train_loader)} train, {len(val_loader)} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719fae1",
   "metadata": {},
   "source": [
    "## 7. Visualize Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch\n",
    "degraded_batch, clean_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch shapes:\")\n",
    "print(f\"   Degraded: {degraded_batch.shape}\")\n",
    "print(f\"   Clean: {clean_batch.shape}\")\n",
    "print(f\"   Range: [{degraded_batch.min():.3f}, {degraded_batch.max():.3f}]\")\n",
    "\n",
    "# Show sample using utility function\n",
    "plot_image_comparison(\n",
    "    degraded_batch=degraded_batch,\n",
    "    clean_batch=clean_batch,\n",
    "    n_samples=4,\n",
    "    save_path=samples_dir / \"training_samples.png\",\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Sample batch saved to {samples_dir / 'training_samples.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632cf910",
   "metadata": {},
   "source": [
    "## 8. Initialize Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    features=config[\"model_features\"],\n",
    "    bilinear=config[\"model_bilinear\"],\n",
    ").to(config[\"device\"])\n",
    "\n",
    "print(\"\\nü§ñ Model: UNet\")\n",
    "print(f\"   Parameters: {model.get_num_params():,}\")\n",
    "print(f\"   Device: {config['device']}\")\n",
    "\n",
    "# Loss function\n",
    "criterion = CombinedLoss(alpha=config[\"loss_alpha\"], beta=config[\"loss_beta\"]).to(\n",
    "    config[\"device\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nüìâ Loss: L1 + SSIM (Œ±={config['loss_alpha']}, Œ≤={config['loss_beta']})\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  Optimizer: AdamW\")\n",
    "print(f\"   Learning rate: {config['learning_rate']}\")\n",
    "print(f\"   Weight decay: {config['weight_decay']}\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if config[\"scheduler\"] == \"cosine\":\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config[\"num_epochs\"] - config[\"warmup_epochs\"],\n",
    "        eta_min=config[\"min_lr\"],\n",
    "    )\n",
    "    print(\"\\nüìÖ Scheduler: CosineAnnealingLR\")\n",
    "    print(f\"   Warmup epochs: {config['warmup_epochs']}\")\n",
    "    print(f\"   Min LR: {config['min_lr']}\")\n",
    "\n",
    "# Resume from checkpoint if enabled\n",
    "start_epoch = 0\n",
    "initial_best_loss = float(\"inf\")\n",
    "\n",
    "if config[\"resume_from_checkpoint\"]:\n",
    "    checkpoint_info, start_epoch = resume_training(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        experiment_path=config[\"resume_experiment\"],\n",
    "        model_name=\"unet\",\n",
    "        degradation=\"gaussian\",\n",
    "        device=config[\"device\"],\n",
    "    )\n",
    "    initial_best_loss = checkpoint_info[\"metrics\"].get(\"val\", {}).get(\"loss\", float(\"inf\"))\n",
    "else:\n",
    "    print(\"\\nüÜï Starting fresh training (resume_from_checkpoint=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f70b34",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb59559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "history, best_info = run_training(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=config[\"device\"],\n",
    "    num_epochs=config[\"num_epochs\"],\n",
    "    checkpoints_dir=checkpoints_dir,\n",
    "    writer=writer,\n",
    "    warmup_epochs=config[\"warmup_epochs\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    patience=config[\"patience\"],\n",
    "    save_every=config[\"save_every\"],\n",
    "    val_every=config[\"val_every\"],\n",
    "    start_epoch=start_epoch,\n",
    "    initial_best_loss=initial_best_loss,\n",
    ")\n",
    "\n",
    "# Extract best model info\n",
    "best_epoch = best_info[\"best_epoch\"]\n",
    "best_val_loss = best_info[\"best_val_loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f12a1d",
   "metadata": {},
   "source": [
    "## 10. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc8834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = save_training_history(history, exp_dir)\n",
    "\n",
    "print(f\"‚úÖ Training history saved to {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1125d",
   "metadata": {},
   "source": [
    "## 11. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves using utility function\n",
    "plot_training_curves(history=history, save_path=exp_dir / \"training_curves.png\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training curves saved to {exp_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f84f7",
   "metadata": {},
   "source": [
    "## 12. Test Inference on Validation Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_info = load_checkpoint(\n",
    "    checkpoints_dir / \"best_model.pth\", model=model, device=config[\"device\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded best model from epoch {checkpoint_info['epoch']+1}\")\n",
    "\n",
    "# Get validation batch\n",
    "degraded_batch, clean_batch = next(iter(val_loader))\n",
    "degraded_batch = degraded_batch.to(config[\"device\"])\n",
    "clean_batch = clean_batch.to(config[\"device\"])\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    restored_batch = model(degraded_batch)\n",
    "\n",
    "# Visualize results using utility function\n",
    "plot_inference_results(\n",
    "    degraded_batch=degraded_batch,\n",
    "    restored_batch=restored_batch,\n",
    "    clean_batch=clean_batch,\n",
    "    n_samples=4,\n",
    "    save_path=samples_dir / \"inference_results.png\",\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Inference results saved to {samples_dir / 'inference_results.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bd2b7",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training summary\n",
    "print_training_summary(\n",
    "    history=history,\n",
    "    best_epoch=best_epoch,\n",
    "    best_val_loss=best_val_loss,\n",
    "    exp_dir=exp_dir,\n",
    "    checkpoints_dir=checkpoints_dir,\n",
    "    samples_dir=samples_dir,\n",
    "    logs_dir=logs_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd374f",
   "metadata": {},
   "source": [
    "## 14. Quantitative Evaluation on Full-Resolution Images\n",
    "\n",
    "Evaluate the model on entire validation images using sliding window inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = ImageRestorationEvaluator(\n",
    "    model=model,\n",
    "    device=config[\"device\"],\n",
    "    patch_size=config[\"patch_size\"],\n",
    "    overlap=32,  # Overlap for smooth blending\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Evaluator configured:\")\n",
    "print(f\"   Patch size: {config['patch_size']}\")\n",
    "print(\"   Overlap: 32 pixels\")\n",
    "print(f\"   Device: {config['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set (limit to 10 images for speed)\n",
    "eval_results = evaluator.evaluate_dataset(\n",
    "    degraded_dir=config[\"val_degraded_dir\"],\n",
    "    clean_dir=config[\"val_clean_dir\"],\n",
    "    output_dir=exp_dir / \"restored_images\",\n",
    "    save_outputs=True,  # Save restored images\n",
    "    max_images=10,  # Change to None to evaluate all images\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "evaluator.print_summary(eval_results)\n",
    "\n",
    "# Save results\n",
    "evaluator.save_results(eval_results, exp_dir / \"evaluation_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8048701",
   "metadata": {},
   "source": [
    "## 15. Visualize Full-Resolution Restoration\n",
    "\n",
    "Show best and worst restoration results on full images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a582f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best and worst images\n",
    "best_img = max(eval_results[\"per_image\"], key=lambda x: x[\"psnr\"])\n",
    "worst_img = min(eval_results[\"per_image\"], key=lambda x: x[\"psnr\"])\n",
    "\n",
    "\n",
    "# Load images for visualization\n",
    "def load_image_trio(filename):\n",
    "    degraded_path = Path(config[\"val_degraded_dir\"]) / filename\n",
    "    clean_path = Path(config[\"val_clean_dir\"]) / filename\n",
    "    restored_path = exp_dir / \"restored_images\" / filename\n",
    "\n",
    "    degraded = cv2.cvtColor(cv2.imread(str(degraded_path)), cv2.COLOR_BGR2RGB)\n",
    "    clean = cv2.cvtColor(cv2.imread(str(clean_path)), cv2.COLOR_BGR2RGB)\n",
    "    restored = cv2.cvtColor(cv2.imread(str(restored_path)), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return degraded, restored, clean\n",
    "\n",
    "\n",
    "# Visualize best result\n",
    "print(f\"\\nüèÜ Best result: {best_img['filename']}\")\n",
    "print(f\"   PSNR: {best_img['psnr']:.2f} dB, SSIM: {best_img['ssim']:.4f}\")\n",
    "\n",
    "deg, res, cln = load_image_trio(best_img[\"filename\"])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(deg)\n",
    "axes[0].set_title(\"Degraded\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(res)\n",
    "axes[1].set_title(f\"Restored\\nPSNR: {best_img['psnr']:.2f} dB\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].imshow(cln)\n",
    "axes[2].set_title(\"Ground Truth\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Best Result: {best_img['filename']}\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(samples_dir / \"best_full_restoration.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize worst result\n",
    "print(f\"\\n‚ö†Ô∏è Worst result: {worst_img['filename']}\")\n",
    "print(f\"   PSNR: {worst_img['psnr']:.2f} dB, SSIM: {worst_img['ssim']:.4f}\")\n",
    "\n",
    "deg, res, cln = load_image_trio(worst_img[\"filename\"])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(deg)\n",
    "axes[0].set_title(\"Degraded\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(res)\n",
    "axes[1].set_title(f\"Restored\\nPSNR: {worst_img['psnr']:.2f} dB\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].imshow(cln)\n",
    "axes[2].set_title(\"Ground Truth\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Worst Result: {worst_img['filename']}\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(samples_dir / \"worst_full_restoration.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Image-Enhancement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
