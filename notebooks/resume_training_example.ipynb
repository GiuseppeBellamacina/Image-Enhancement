{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Aggiungere la directory src al path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models import UNet\n",
    "from losses import CombinedLoss\n",
    "from training import get_dataloaders, run_training\n",
    "from utils import (\n",
    "    setup_experiment,\n",
    "    resume_training,\n",
    "    get_degraded_data_dir,\n",
    "    get_raw_data_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8191a",
   "metadata": {},
   "source": [
    "## Configurazione con Resume Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27200d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è CONFIGURATION\n",
    "config = {\n",
    "    # üîÑ RESUME TRAINING - Imposta questo flag!\n",
    "    \"resume_from_checkpoint\": True,   # ‚Üê Cambia a True per riprendere training\n",
    "    \"resume_experiment\": \"latest\",     # \"latest\" o timestamp specifico come \"20251229_224726\"\n",
    "    \n",
    "    # Data paths\n",
    "    \"train_degraded_dir\": str(get_degraded_data_dir() / \"gaussian\" / \"DIV2K_train_HR\"),\n",
    "    \"train_clean_dir\": str(get_raw_data_dir() / \"DIV2K_train_HR\"),\n",
    "    \"val_degraded_dir\": str(get_degraded_data_dir() / \"gaussian\" / \"DIV2K_valid_HR\"),\n",
    "    \"val_clean_dir\": str(get_raw_data_dir() / \"DIV2K_valid_HR\"),\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"batch_size\": 16,\n",
    "    \"num_epochs\": 50,  # ‚Üê Totale epoche (include quelle gi√† fatte se resume)\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \n",
    "    # Model\n",
    "    \"model_features\": 64,\n",
    "    \"model_bilinear\": True,\n",
    "    \n",
    "    # Optimizer\n",
    "    \"scheduler\": \"cosine\",\n",
    "    \"warmup_epochs\": 5,\n",
    "    \"min_lr\": 1e-6,\n",
    "    \"patience\": 5,\n",
    "    \n",
    "    # Device\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "print(\"üìã Configurazione:\")\n",
    "print(f\"   Resume from checkpoint: {config['resume_from_checkpoint']}\")\n",
    "if config['resume_from_checkpoint']:\n",
    "    print(f\"   Resume experiment: {config['resume_experiment']}\")\n",
    "print(f\"   Device: {config['device']}\")\n",
    "print(f\"   Total epochs: {config['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbc010",
   "metadata": {},
   "source": [
    "## Setup Esperimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c83fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se non stai facendo resume, crea un nuovo esperimento\n",
    "# (altrimenti lo riutilizzerai quello esistente)\n",
    "if not config[\"resume_from_checkpoint\"]:\n",
    "    exp_dir, subdirs = setup_experiment(\n",
    "        model_name=\"unet\",\n",
    "        degradation=\"gaussian\",\n",
    "        config=config,\n",
    "        custom_name=\"v1\"\n",
    "    )\n",
    "    checkpoints_dir = subdirs[\"checkpoints\"]\n",
    "    logs_dir = subdirs[\"logs\"]\n",
    "else:\n",
    "    # Quando fai resume, userai le directory dell'esperimento esistente\n",
    "    print(\"‚è≠Ô∏è  Skipping new experiment setup (will use existing from resume)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d52cba",
   "metadata": {},
   "source": [
    "## Crea DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_dataloaders(\n",
    "    train_degraded_dir=config[\"train_degraded_dir\"],\n",
    "    train_clean_dir=config[\"train_clean_dir\"],\n",
    "    val_degraded_dir=config[\"val_degraded_dir\"],\n",
    "    val_clean_dir=config[\"val_clean_dir\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    patch_size=128,\n",
    "    patches_per_image=10,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders created: {len(train_loader)} train batches, {len(val_loader)} val batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60159af",
   "metadata": {},
   "source": [
    "## Inizializza Model, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a97f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea modello, optimizer e scheduler come al solito\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    features=config[\"model_features\"],\n",
    "    bilinear=config[\"model_bilinear\"],\n",
    ").to(config[\"device\"])\n",
    "\n",
    "criterion = CombinedLoss(alpha=0.84, beta=0.16).to(config[\"device\"])\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config[\"num_epochs\"] - config[\"warmup_epochs\"],\n",
    "    eta_min=config[\"min_lr\"],\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model created: {model.get_num_params():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0b17c",
   "metadata": {},
   "source": [
    "## üîÑ Resume Logic - QUESTO √à IL PUNTO CHIAVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabili per tracking\n",
    "start_epoch = 0\n",
    "initial_best_loss = float(\"inf\")\n",
    "\n",
    "if config[\"resume_from_checkpoint\"]:\n",
    "    # üîÑ RESUME: Carica tutto automaticamente\n",
    "    checkpoint_info, start_epoch = resume_training(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        experiment_path=config[\"resume_experiment\"],\n",
    "        model_name=\"unet\",\n",
    "        degradation=\"gaussian\",\n",
    "        device=config[\"device\"],\n",
    "    )\n",
    "    \n",
    "    # Estrai best loss precedente\n",
    "    initial_best_loss = checkpoint_info[\"metrics\"].get(\"val\", {}).get(\"loss\", float(\"inf\"))\n",
    "    \n",
    "    # Usa le directory dell'esperimento caricato\n",
    "    # (estrai il path dall'esperimento caricato)\n",
    "    from utils import get_model_experiments_dir\n",
    "    base_dir = get_model_experiments_dir(\"unet\", \"gaussian\")\n",
    "    \n",
    "    if config[\"resume_experiment\"] == \"latest\":\n",
    "        experiment_dirs = sorted(\n",
    "            [d for d in base_dir.iterdir() if d.is_dir()],\n",
    "            key=lambda x: x.name,\n",
    "            reverse=True,\n",
    "        )\n",
    "        exp_dir = experiment_dirs[0]\n",
    "    else:\n",
    "        exp_dir = base_dir / config[\"resume_experiment\"]\n",
    "    \n",
    "    checkpoints_dir = exp_dir / \"checkpoints\"\n",
    "    logs_dir = exp_dir / \"logs\"\n",
    "    \n",
    "    print(f\"\\n‚úÖ Resuming from: {exp_dir}\")\n",
    "    print(f\"   Start epoch: {start_epoch}\")\n",
    "    print(f\"   Previous best loss: {initial_best_loss:.4f}\")\n",
    "    \n",
    "else:\n",
    "    # üÜï NUOVO TRAINING\n",
    "    print(\"\\nüÜï Starting fresh training\")\n",
    "    print(f\"   Experiment: {exp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d34ca",
   "metadata": {},
   "source": [
    "## TensorBoard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=logs_dir)\n",
    "print(f\"üìä TensorBoard logs: {logs_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2dee9",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training con i parametri di resume\n",
    "history, best_info = run_training(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=config[\"device\"],\n",
    "    num_epochs=config[\"num_epochs\"],\n",
    "    checkpoints_dir=checkpoints_dir,\n",
    "    writer=writer,\n",
    "    warmup_epochs=config[\"warmup_epochs\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    patience=config[\"patience\"],\n",
    "    save_every=5,\n",
    "    val_every=1,\n",
    "    # ‚¨áÔ∏è PARAMETRI CRITICI PER RESUME\n",
    "    start_epoch=start_epoch,\n",
    "    initial_best_loss=initial_best_loss,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completato!\")\n",
    "print(f\"   Best epoch: {best_info['best_epoch']}\")\n",
    "print(f\"   Best loss: {best_info['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138e810",
   "metadata": {},
   "source": [
    "## Riepilogo\n",
    "\n",
    "### Per iniziare un nuovo training:\n",
    "```python\n",
    "config[\"resume_from_checkpoint\"] = False\n",
    "```\n",
    "\n",
    "### Per riprendere l'ultimo training:\n",
    "```python\n",
    "config[\"resume_from_checkpoint\"] = True\n",
    "config[\"resume_experiment\"] = \"latest\"\n",
    "```\n",
    "\n",
    "### Per riprendere un training specifico:\n",
    "```python\n",
    "config[\"resume_from_checkpoint\"] = True\n",
    "config[\"resume_experiment\"] = \"20251229_224726\"  # timestamp specifico\n",
    "```\n",
    "\n",
    "**Tutto viene caricato automaticamente**: modello, optimizer, scheduler, epoch, best loss! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
